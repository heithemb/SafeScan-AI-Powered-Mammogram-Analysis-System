{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file contains a K-means algorithm used to find the optimal anchor sizes for Mask R-CNN training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "link to our dataset on roboflow:\n",
        "https://universe.roboflow.com/segmentationcmmd/segmentation-calc-mass-aarac\n",
        "original dataset:\n",
        "https://www.cancerimagingarchive.net/analysis-result/tompei-cmmd/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osyjIFyGAiFL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import CocoDetection\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pycocotools import mask as maskUtils\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRqAG0V9Aj4p"
      },
      "outputs": [],
      "source": [
        "dataset_root = \"/content/segmentation-calc-mass-12/\"\n",
        "ann_fileTr = os.path.join(dataset_root, \"train/_annotations.coco.json\")\n",
        "ann_fileTe = os.path.join(dataset_root, \"test/_annotations.coco.json\")\n",
        "ann_fileV = os.path.join(dataset_root, \"valid/_annotations.coco.json\")\n",
        "img_folderTr = os.path.join(dataset_root, \"train/\")\n",
        "img_folderTe = os.path.join(dataset_root, \"test/\")\n",
        "img_folderV = os.path.join(dataset_root, \"valid/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrENd3cxAl7O",
        "outputId": "0e20d20d-60c8-47eb-e3ed-40a15f2250b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.74s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "class CustomCocoDataset(CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transform=None):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self.transform = transform\n",
        "        self.img_folder = img_folder\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        img_info = self.coco.loadImgs(self.ids[idx])[0]\n",
        "        img_path = os.path.join(self.img_folder, img_info[\"file_name\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        image_id = self.ids[idx]  # Get the COCO image ID\n",
        "\n",
        "        # Convert COCO annotations to Mask R-CNN format\n",
        "        boxes = []\n",
        "        masks = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in target:\n",
        "            xmin, ymin, w, h = ann[\"bbox\"]\n",
        "            xmax, ymax = xmin + w, ymin + h\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "\n",
        "            # Handle segmentation masks\n",
        "            if isinstance(ann[\"segmentation\"], list):  # Polygon format\n",
        "                mask = np.zeros((img.height, img.width), dtype=np.uint8)\n",
        "                for seg in ann[\"segmentation\"]:\n",
        "                    poly = np.array(seg, dtype=np.int32).reshape((-1, 2))\n",
        "                    cv2.fillPoly(mask, [poly], 1)\n",
        "            else:  # RLE format\n",
        "                mask = maskUtils.decode(ann[\"segmentation\"])\n",
        "            masks.append(mask)\n",
        "\n",
        "        # Convert to tensors\n",
        "        if boxes:\n",
        "            boxes = torch.as_tensor(np.array(boxes, dtype=np.float32), dtype=torch.float32)\n",
        "            labels = torch.as_tensor(np.array(labels, dtype=np.int64), dtype=torch.int64)\n",
        "            masks = torch.as_tensor(np.array(masks, dtype=np.uint8), dtype=torch.uint8)\n",
        "        else:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            masks = torch.zeros((0, img.height, img.width), dtype=torch.uint8)\n",
        "\n",
        "        target_dict = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": torch.tensor([image_id]),  # Add image_id as tensor\n",
        "            \"size\": torch.tensor([img.height, img.width])  # Useful for evaluation\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target_dict\n",
        "\n",
        "# Define transformation\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = CustomCocoDataset(img_folderTr, ann_fileTr, transform=transform)\n",
        "val_dataset = CustomCocoDataset(img_folderV, ann_fileV, transform=transform)\n",
        "test_dataset = CustomCocoDataset(img_folderTe, ann_fileTe, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyFnFf6ExJl"
      },
      "source": [
        "957x1147 size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqmtraeqAMIM",
        "outputId": "f98cbdf3-89f8-4898-ca6f-c3f1e24133b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom anchors configured for FPN:\n",
            "Sizes per level: [np.float32(67.5), np.float32(126.1), np.float32(191.9), np.float32(277.5), np.float32(407.9)]\n",
            "Aspect ratios: (0.5, 1.0, 2.0)\n"
          ]
        }
      ],
      "source": [
        "# 1) Collect box dimensions from TRAINING DATA ONLY\n",
        "all_wh = []\n",
        "\n",
        "for image, target in train_dataset:  # Only use train_dataset!\n",
        "    boxes = target['boxes']\n",
        "    widths = (boxes[:, 2] - boxes[:, 0]).cpu().numpy()\n",
        "    heights = (boxes[:, 3] - boxes[:, 1]).cpu().numpy()\n",
        "    all_wh.append(np.stack([widths, heights], axis=1))\n",
        "\n",
        "all_wh = np.vstack(all_wh)  # Shape: (total_boxes, 2)\n",
        "\n",
        "# 2) K-means clustering for anchor sizes\n",
        "NUM_CLUSTERS = 5  # Matches typical FPN levels\n",
        "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
        "kmeans.fit(all_wh)\n",
        "anchors = kmeans.cluster_centers_\n",
        "\n",
        "# 3) Prepare for FPN integration\n",
        "# Sort anchors by area (width*height)\n",
        "anchors = anchors[anchors.prod(axis=1).argsort()]\n",
        "\n",
        "# Convert to base sizes (sqrt(wh)) for AnchorGenerator\n",
        "anchor_sizes = tuple((np.sqrt(w * h),) for w, h in anchors)  # One size per FPN level\n",
        "\n",
        "# Use standard aspect ratios (better than clustering ratios)\n",
        "ASPECT_RATIOS = (0.5, 1.0, 2.0)  # Same for all FPN levels\n",
        "aspect_ratios = (ASPECT_RATIOS,) * NUM_CLUSTERS\n",
        "\n",
        "print(\"Custom anchors configured for FPN:\")\n",
        "print(f\"Sizes per level: {[round(s[0], 1) for s in anchor_sizes]}\")\n",
        "print(f\"Aspect ratios: {ASPECT_RATIOS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr97k6cwBYQB",
        "outputId": "c6e3129b-393a-408c-f5b8-85e2c0466263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean IoU: 0.70\n"
          ]
        }
      ],
      "source": [
        "def compute_ious(anchors, gt_wh):\n",
        "    a_areas = anchors.prod(axis=1)\n",
        "    gt_areas = gt_wh.prod(axis=1)\n",
        "\n",
        "    inter_w = np.minimum(anchors[:,0], gt_wh[:,0,None])\n",
        "    inter_h = np.minimum(anchors[:,1], gt_wh[:,1,None])\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "    ious = inter / (a_areas + gt_areas[:,None] - inter)\n",
        "    return ious.max(axis=1).mean()  # Mean best IoU\n",
        "\n",
        "print(f\"Mean IoU: {compute_ious(anchors, all_wh):.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
